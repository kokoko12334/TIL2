from itertools import groupby
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import pandas as pd

iris = load_iris()

x = iris['data'][:,0] 
y =iris['target']
data = pd.DataFrame(data = np.c_[x,y], columns = ['x','y'])
data.sort_values(by='x', inplace=True)


elements = list(data['x'].unique())

#지니지수 초기값
first_gini = 1
for i in data['y'].value_counts():
    first_gini -= (i/len(data))**2
print(first_gini)

#구간별로 평균값 구하기
categories = []
for i in range(len(elements)):
    try:
        categories.append((elements[i]+elements[i+1])/2)
    except:
        break
print(categories)

#지니 지수 구하는 함수
def gini(data):
    cnt = data['y'].value_counts()
    g=1
    for i in cnt:
        g -= (i/len(data))**2
    return g

#앞에서 구한 평균값으로 구분하고 지니지수의 가중평균 구하기
#어떤 기준으로 구간이 두개가 나뉘어지는데 두개의 지니지수가 나온다.
#이때 가중평균을 이용하여 지니지수를 한개의 지표로 표시한다.
w_gini = []
for c in categories:

    condition = data['x'] >= c
    d_l = data[condition]
    d_r = data[~condition]
    
    gini_l = gini(d_l)        
    gini_r = gini(d_r)
    
    w = (len(d_l)/len(data))*gini_l+(len(d_r)/len(data))*gini_r 
    w_gini.append(w)

#ig구하기
ig = [first_gini-i for i in w_gini]

r=dict(zip(ig,categories))

print(r[max(r)])



#################################강사
# -*- coding: utf-8 -*-
"""tree_분기.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FEV3AzR0Z_j0bsD6Z4Po2EbU0SW8O-WB
"""

from sklearn.datasets import load_iris
import numpy as np
import pandas as pd

# iris 데이터를 읽어온다
iris = load_iris()

# iris['data']의 첫번째 feature만 사용하고, dataframe으로 저장한다.
df = pd.DataFrame(data= np.c_[iris['data'][:, 0], iris['target']], columns= ['x', 'y'])

# target의 class (label)는 0, 1, 2
labels = set(df['y'])

# 중복된 데이터를 제거하고 오름차순으로 정렬한다.
x_feat = list(set(df['x']))
x_feat.sort()

# split_points 리스트를 계산해 둔다 (분기 조건에 대한 후보들이다).
split_points = [np.mean([x_feat[i-1], x_feat[i]]) for i in range(1, len(x_feat))]

# Leaf node의 Gini index를 계산한다.
# L : left node, R : right node
def calculate_gini(L, R):
    n_left = np.sum(L)
    n_right = np.sum(R)
    n_total = n_left + n_right

    g_left = 1 - ((L / n_left) ** 2).sum()
    g_right = 1 - ((R / n_right) ** 2).sum()
    
    # weighted average
    gini = g_left * n_left / n_total + g_right * n_right / n_total
    return gini

# 모든 split_points에 대해 Gini index를 계산한다.
gini = []
for s in split_points:
    left = np.array(df[df['x'] <= s]['y']).astype('int')
    left_leaf = [np.sum(left == x) for x in labels]

    right = np.array(df[df['x'] > s]['y']).astype('int')
    right_leaf = [np.sum(right == x) for x in labels]

    gini.append(calculate_gini(left_leaf, right_leaf))

# split_points 중에 최적 point를 찾는다.
# Gini index가 가장 작은 point를 찾는다. 이 지점의 information gain이 가장 크다.
best = np.argmin(gini)
print("Best split point = {}".format(split_points[best]))

from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt

# sklearn으로 decision tree를 생성한다.
x_train = np.array(df['x']).reshape(-1, 1)
y_train = df['y']

# 최초의 분기만 확인하기 위해 max_depth = 1로 설정한다.
clf = DecisionTreeClassifier(max_depth=1)
clf.fit(x_train, y_train)

# 결과를 확인한다.
plt.figure(figsize=(12,4))
tree.plot_tree(clf, feature_names = ['x'], fontsize=12)
plt.show()



##############라이브러리
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt

iris = load_iris()
df = pd.DataFrame(data= np.c_[iris['data'][:, 0], iris['target']], columns= ['x', 'y'])


x_train = np.array(df['x']).reshape(-1,1)
y_train = df['y']



clf = DecisionTreeClassifier(max_depth=1)
clf.fit(x_train, y_train)

tree.plot_tree(clf, feature_names=['x'], fontsize=15)
plt.show()



#########실습과제 모든 feature 사용함.

from sklearn.model_selection import train_test_split

iris = load_iris()

x_train, x_test, y_train, y_test = train_test_split(iris['data'], iris['target'], test_size=0.2)


clf = DecisionTreeClassifier(max_depth=2)
clf.fit(x_train, y_train)

#train data로 의사결정 나무 그림
tree.plot_tree(clf, fontsize=10)
plt.show()
#예측값
y_pred=clf.predict(x_test)
#비교
clf.score(x_test, y_test)


#####depth에 따른 정확도
iris = load_iris()

x_train, x_test, y_train, y_test = train_test_split(iris['data'], iris['target'], test_size=0.2)

test = range(1,21)
result = []
for i in test:
    clf = DecisionTreeClassifier(max_depth=i)
    clf.fit(x_train, y_train)
    r = clf.score(x_test, y_test)
    result.append(r)

plt.plot(test, result, 'o--')
plt.xlim = (1,21)
plt.ylim = (0,1.1)
plt.xlabel('depth')
plt.ylabel('score')
plt.show()


clf.predict(np.array([[1,2,3,4]]))




#################DT regression

# -*- coding: utf-8 -*-
"""DTree(regression).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f-WaoMeN7VZUEFWYKw-NuEATd_mrnIRP
"""

# Regression tree 연습
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split

x = np.arange(1, 19).reshape(-1, 1)
y = np.array([4.14, 3.37, 11.7, 6.3, 10.25, 10.32, 
              5.67, 7.12, 12.47, 17.97, 19.34, 21.62, 
              15.64, 25.83, 29.28, 21.47, 26.3, 32.48])

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

plt.scatter(x, y, c='red', s=100)
plt.show()

# regression tree를 생성한다.
reg = DecisionTreeRegressor(max_depth=5)
reg.fit(x, y)

print('R2 = ', reg.score(x_test, y_test))

# 임의의 x에 대한 y값을 추정한다.
a = 10.5
x_new = np.array(a).reshape(-1, 1)
y_hat = reg.predict(x_new)
print("\nx = {}일 때 y의 추정값 = {:.4f}".format(a, y_hat[0]))

# 학습 데이터인 x에 대한 y를 다시 추정해 보고, 원래 y와 비교해 본다.
y_hat = reg.predict(x)

# 추정된 y를 시각화 한다.
plt.scatter(x, y, c='red', s=100, alpha=0.7)
plt.plot(x, y_hat, marker='o', c='blue', alpha=0.7, 
         drawstyle="steps-post")
plt.show()





#########################FM############
#depth를 추정하는데 test data를 쓰게 된다. 이때 test는 test의 용도로 남기기 위해서
#train data에서 다시 나누어서 train , evaluatuin으로 나누고
#train으로 학습, evaluation으로 depth를 결정하고 만들어진 모형으로 test를 실행한다.
#즉  .fit(x_train, y_train)      =>       .score(x_eval, y_eval)  (depth를 평가하는 지표이므로)






